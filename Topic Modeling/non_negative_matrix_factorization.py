# -*- coding: utf-8 -*-
"""Non-Negative Matrix Factorization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12-2WiHewhvzQan87Z7LP-23IPFUR3tFx

Non-negative Matrix Factorization is an unsupervised algorithm that
simultaneously performs dimenstionality reduction and clustering.

we can use it in conjuction with TF-IDF to model topics across
documents.


General mathematic behind non-negative matrix factorization or NMF:

Given a non-negative matrix A, which is what we are going to be able to
create using TF-IDF.
And then we want to find a k-dimention approximation in
terms of non-negative factors W and H.

So we are gonna have matrix A, which n*m some rows features and columns
or objects.

And we wanna perform factorization to essentially approximate A with the
matrix multiplication of W and H, where we have N by K for W

A(n*m: Data Matrix:- Rows=Features, Cols=Objects)-->
W(n*k: Basis Vectors :- Rows=Features) * H(k*m: Coefficient Matrix:Cols =objects)

Approximate each object(i.e. column of A) by a linear combination of k
reduced dimetions or "bais vectors" in W.

Each basis vector can be interpreted as a cluster. The memberships of
objects in these clusters encoded by H.

Input:- Non-negative data matrix(A), number of basis vectors(k), initial
values of factors W and H (e.g random matrices)

Objective Function:- Some measure of reconstruction error between A
and the approximation WH.

Expectation-maximization optimisation to refine W and H in order to
minimise the objective function. Common approach is to itertate between
two multiplicative update rules until convergence approximation of WH
is going to make sense for A

1.) Construct vector space model for documents(after stopword filtering),
resulting in a term document matrix A.

2.) Apply TF-IDF term weight to normalisation to A.

3.) Normalize TF-IDF vetros to unit length.

4.) Initialise factors using NNDSVD on A.
NNDSVD:- Non-negative double single singular value decomposition


5.) Apply Projected Gradient NMF to A.

We end up discovering:-

Basis vectors:- The topics(clusters) in the data.

Coefficient matrix:- The membership weights for documents relative to
each topic(cluster)

Just like LDA, we will need to select the number of expected topics
beforehand (the value of k)!

Also just like with LDA, we will have to interpret the topics based off
the coefficient values of the words per topic.


""
"""

import pandas as pd

npr=pd.read_csv("/content/npr.csv")

npr.head()

from sklearn.feature_extraction.text import TfidfVectorizer

tfidf=TfidfVectorizer(max_df=0.95,min_df=2,stop_words="english")
tfidf

dtm=tfidf.fit_transform(npr['Article'])
dtm

from sklearn.decomposition import NMF

nmf_model=NMF(n_components=7,random_state=42)
nmf_model

nmf_model.fit(dtm)

# display topics

tfidf.get_feature_names_out()[2300]

for index,topic in enumerate(nmf_model.components_):
    print(f"The top 15 words for the topic #{index}")
    print([tfidf.get_feature_names_out()[i] for i in topic.argsort()[-15:]])
    print("\n")

# attach topic labels to articles

topic_results=nmf_model.transform(dtm)
topic_results

topic_results[0].argmax()

npr['Topic']=topic_results.argmax(axis=1)
npr.head()

topic_dict={0:'Health',1:'Campaign',2:'legislation',3:'Politics',\
            4:'election',5:'music',6:'education'}

npr['Topic Label']=npr['Topic'].map(topic_dict)
npr.head(15)

npr['Topic Label'].value_counts()