# -*- coding: utf-8 -*-
"""Latent Dirichlet Allocation with Python.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QgYSICCe5jE4oTMHF6Qq3AiCMnXeJt-P
"""

import pandas as pd

npr = pd.read_csv("/content/npr.csv")
npr.head()

npr.info()

npr.isnull().sum()

npr['Article'][1]

from sklearn.feature_extraction.text import CountVectorizer

cv= CountVectorizer(max_df=0.9, min_df=2,stop_words='english')
cv

# max_df: going to ignore documents with high frequency.
# it gets rid of terms that are really common across a lot of the documents.
# we can pass number between 0 and 1.
# if we select 0.9 means it's going to discard words that show up in
# 90% of the documents.

# min_df: which is essentially a minimum document frequency
# So words that show up a minimum amount of times

dtm = cv.fit_transform(npr['Article'])
dtm

from sklearn.decomposition import LatentDirichletAllocation

LDA = LatentDirichletAllocation(n_components=7,random_state=42)
LDA

LDA.fit(dtm)

# Grab the vocabulary of words

len(cv.get_feature_names_out())

type(cv.get_feature_names_out())

cv.get_feature_names_out()[1000]

cv.get_feature_names_out()[10000]

import random

random_word_id=random.randint(0,54777)

cv.get_feature_names_out()[random_word_id]
# keep running this cell and we get different word each time

# Grab the topics
len(LDA.components_)

type((LDA.components_))

LDA.components_.shape

LDA.components_

single_topic=LDA.components_[0]
single_topic

# argsort() it return the index positions that will sort this array.

single_topic.argsort()

import numpy as np
arr=np.array([1,100,10])
arr.argsort()

# argsort -- index positions sorted from least to greatest
# top 10 values(10 greatest values)
# last 10 values of argsort()

single_topic.argsort()[-10:]

top_ten_words =single_topic.argsort()[-10:]

for index in top_ten_words:
    print(cv.get_feature_names_out()[index])

top_twenty_words =single_topic.argsort()[-20:]
for index in top_twenty_words:
    print(cv.get_feature_names_out()[index])

# These words has higher probabilites of showing up in this particular
# single topic

# top 15 words from each 7 topics

for i,topic in enumerate(LDA.components_):
    print(f"The top 15 words for topic #{i}")
    print([cv.get_feature_names_out()[index] for index in topic.argsort()[-15:]])
    print("\n")

# 10 highest probability words to show up in that particular topic

# Grab the highest probability words per topic

dtm

npr.head()

topic_results=LDA.transform(dtm)
topic_results

topic_results.shape

topic_results[0]

topic_results[0].round(2)

# 1st article has probability of 68% belongs to politics

# lets check...

npr['Article'][1]

# index position of highest probability

topic_results[0].argmax()

npr['Topic']=topic_results.argmax(axis=1)
npr.head(10)