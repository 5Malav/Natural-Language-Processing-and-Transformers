{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling"
      ],
      "metadata": {
        "id": "bwHl_-C-vLO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Topic Modeling allows for us to efficiently analyze large volumes of\n",
        "text by clustering documents into topics.\n",
        "\n",
        "A large amount of text data in unlabeled meaning we will not be able\n",
        "to apply Supervised machine learning models would actually depend\n",
        "on historical label data.\n",
        "\n",
        "And in the real world, specifically for text data, we are not gonna\n",
        "have a convenient label attached to a text data set, such as\n",
        "positve or negative, or spam vs. ham.\n",
        "\n",
        "Instead , we may have variety of labels such as different categories\n",
        "that a newspaper article could be in and we may just have the test\n",
        "itself unlabeled.\n",
        "\n",
        "So its up to us to try to discover those labels through Topic Modeling.\n",
        "\n",
        "If we have unlabeled data, then we can attempt to \"discover\" labels.\n",
        "\n",
        "In the case of text data, this means attempting to discover clusters\n",
        "of attempting to discover clusters of documents, grouped together by \\\n",
        "topic.\n",
        "\n",
        "A very important to keep in mind here is that we don't know the \"correct\"\n",
        "topic or \"right anwer\"!\n",
        "\n",
        "All we know is that the document clustered together share similar topic\n",
        "ideas.\n",
        "\n",
        "It is up to us to identify what these topics really represent.\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "uZOQo7IJoorF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Latent Dirichlet Allocation"
      ],
      "metadata": {
        "id": "bT48tvxkymWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "Johann Peter Gustav Lejeune Dirichlet was a German mathmatician in the\n",
        "1980s who contributed widely to the field of modern mathematics.\n",
        "\n",
        "There is a probability distribution named after him \"Dirichilet Distribution\"\n",
        "\n",
        "Latent Dirichlet Allocation is based off this probability distribution.\n",
        "\n",
        "In 2003 LDA was first published as a graphical model for topic discovery\n",
        "a Journal of Machine Learning Research by David Blei, Andrew Ng and\n",
        "Michael I. Jordan.\n",
        "\n",
        "Assumptions of LDA for TOpic Modeling:-\n",
        "\n",
        "1.) Documents with similar topics use similar groups of words.\n",
        "\n",
        "2.) Latent topics can then be found by searching for groups of words that\n",
        "frequently occure together in documents across the corpus.\n",
        "\n",
        "Think of these 2 assumptions mathematically.\n",
        "\n",
        "Documents are probabiluity distributions over latent topics.\n",
        "\n",
        "Topics themselves are probability distributions over words.\n",
        "\n",
        "We can imagine that any particular document is going to have a\n",
        "probability distribution over a given amount of latent topics.\n",
        "\n",
        "So let's say, we decide that there are five latent topics across\n",
        "various documents.\n",
        "\n",
        "Then any particular document is going to have a probability of\n",
        "belonging to each topic.\n",
        "\n",
        "Topics themselves are probability distributions over words.\n",
        "\n",
        "LDA represents documents as mixtures of topics that spit out words\n",
        "with certain probabilities\n",
        "\n",
        "It assumes that documents are produced in the following fashion:1.\n",
        "\n",
        "1.) Decide on the number of words N the document will have.\n",
        "\n",
        "2.) Choose a topic mixture for the document(according to a Dirichilet\n",
        "distribution over a fixed set of K topics.)\n",
        "For example:- 60% business, 20%v politics and 10% food.\n",
        "\n",
        "Generate each word in the document by:\n",
        "\n",
        "1.) FIrst picking a topic according to the multinomial distribution that\n",
        "we sampled previosuly (60% business, 20% politics, 10% food)\n",
        "\n",
        "2.) Using the topic to genrate the word itself(according to the topic's\n",
        "multinomial distribution)\n",
        "\n",
        "For example, if we selected food topic, we might generate the word\n",
        "\"apple\" with 60% probability , \"home\" with 30% probability and so on.\n",
        "\n",
        "Assuming this generative model for a collection of documents , LDA then\n",
        "tries to backtrack from the document to find a set of topics that are\n",
        "likely to have generated collection.\n",
        "\n",
        "Now imagine we have a set of documents.\n",
        "\n",
        "we have chosen some fixed number of K topics to discover, and want to\n",
        "use LDA to learn the topic representation of each document and the words\n",
        "associated to each topic.\n",
        "\n",
        "Go through each document, and randomly assign each word in the\n",
        "document to one of the K topics.\n",
        "\n",
        "This random assignment already gives you both topic representations\n",
        "of all the document and word distribution of all the topics.\n",
        "(Note:- This initial topics won't make sense)\n",
        "They are gonnna be really poor representation of topics since we\n",
        "just essentially assigned every word a random topic.\n",
        "\n",
        "Now we iterate over every word in every document to improve these topics.\n",
        "\n",
        "For every word in every document and for each topic t we calculate:\n",
        "\n",
        "p(topic t | document d) = the proportion of word in document d that\n",
        "are currently assigned to topic t.\n",
        "\n",
        "p(word w | topic t) =  the proportion of assignements to topic t over\n",
        "all documents that come from this word w.\n",
        "\n",
        "Reassign w a new topic, where we choose topic t with probability\n",
        "p(topic t | document d) * p(word w | topic t)\n",
        "\n",
        "This nis the probability that topic t generated word w.\n",
        "\n",
        "After repeating the previous step a large number of times, we\n",
        "eventually reach a roughly state where the assignment are acceptable.\n",
        "\n",
        "At the end we have each document assigned to a topic.\n",
        "\n",
        "We also can search for the words that have the highest probility of being\n",
        "assigned to a topic.\n",
        "\n",
        "We end up with an output such as:\n",
        "\n",
        "Document assigned to Topic #4.\n",
        "\n",
        "Most common words(highest probability) for Topic #4.\n",
        "[\"cat\", \"vet\", \"birds\", \"dog\" ...,\"food\",\"home\"]\n",
        "\n",
        "It is upto us to interprete these topics.\n",
        "\n",
        "Two important notes:\n",
        "\n",
        "The use must decide on the amount of topics present in the document.\n",
        "\n",
        "The use must interpret what the topics are.\n",
        "\n",
        "\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "tIYv8jiGoonr",
        "outputId": "6f2861c5-e7eb-48df-b892-c4318c249895"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nJohann Peter Gustav Lejeune Dirichlet was a German mathmatician in the\\n1980s who contributed widely to the field of modern mathematics.\\n\\nThere is a probability distribution named after him \"Dirichilet Distribution\"\\n\\nLatent Dirichlet Allocation is based off this probability distribution.\\n\\nIn 2003 LDA was first published as a graphical model for topic discovery\\na Journal of Machine Learning Research by David Blei, Andrew Ng and\\nMichael I. Jordan.\\n\\nAssumptions of LDA for TOpic Modeling:-\\n\\n1.) Documents with similar topics use similar groups of words.\\n\\n2.) Latent topics can then be found by searching for groups of words that\\nfrequently occure together in documents across the corpus.\\n\\nThink of these 2 assumptions mathematically.\\n\\nDocuments are probabiluity distributions over latent topics.\\n\\nTopics themselves are probability distributions over words.\\n\\nWe can imagine that any particular document is going to have a\\nprobability distribution over a given amount of latent topics.\\n\\nSo let\\'s say, we decide that there are five latent topics across \\nvarious documents.\\n\\nThen any particular document is going to have a probability of\\nbelonging to each topic.\\n\\nTopics themselves are probability distributions over words.\\n\\nLDA represents documents as mixtures of topics that spit out words\\nwith certain probabilities\\n\\nIt assumes that documents are produced in the following fashion:1.\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    }
  ]
}