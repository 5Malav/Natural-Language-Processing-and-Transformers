# -*- coding: utf-8 -*-
"""Part -2 Tokenization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vJcb0DoehVzc820MjNcfUBvJg4cRM2Hk

#Tokenization

-> Tokenization is the process of breaking up the original text into compoenent peices(token)

-> Tokens are pieces of the original text.

-> Tokens are the basic building blocks of a Doc object- everything that helps us understand the meaning of the text is derived from token and their relationship to one another.

-> Prefix : Character(s) at the beginning ( $ " ?)

-> Suffix : Character(s) at the end( km  , ) . ! ")

-> Infix : Character(s) in between ( - --/ ...)

-> Exception : Special case rule to split a string into several tokens or prevent a token from being split when punctuation rules are applied.
"""

import spacy

nlp= spacy.load("en_core_web_sm")

my_string = "'we\'re moving to India next year!"
my_string

doc = nlp(my_string)

for token in doc:
    print(token.text)

doc2 = nlp(u"I am learning PyTorch. for remote job my\
 email id is malavmjoshi@gmail.com, I am also focusing\
 NLP and Computer Vision as well!!!")

for token in doc2:
    print(token.text)

doc3= nlp(u"A cab from Ahmedabad to Deesa costs $200.45")

for token in doc3:
    print(token.text)

doc4 = nlp(u"I will visit my college, st. xavier's next year")

for token in doc4:
    print(token.text)

len(doc4)

doc4.vocab

len(doc4.vocab)

# 799

# this means when language library was load up
# English core web sm for small, that has a vocabulary of
# 799 different types of tokens

# Token can also be retrieved by index position and slice.

#
doc5= nlp(u"It is better to give than receive.")

doc5[0]

doc5[3:6]

# Token cannnot be re-assigned.
# ALthought document objects can be considered lists of tokens
# they do not support re-assignment.
# Once document is created its gonna be fixed.


#doc5[0]="we"
#TypeError: 'spacy.tokens.doc.Doc' object does not support item assignment

# Spacy can also understand named entities.
# Name entities add another layer of context
# The language model that loaded intially recognizes
# that certain words are organization names while other
# are locations and still other combinations relate
# to things like money or dates.

doc8 = nlp(u'Apple this year did $7 billion worth business\
in India'
)

for token in doc8:
    print(token.text, end=" |")

for entity in doc8.ents:
    print(entity)

# Spcay smart enought to recognize that there is something
# spcial about Apple,India, this year,$7 billion
# These are name entities

for entity in doc8.ents:
    print(entity)
    print(entity.label_)
    print("\n")

for entity in doc8.ents:
    print(entity)
    print(entity.label_)
    print(str(spacy.explain(entity.label_)))
    print("\n")

doc9 = nlp(u'Autonomous cars shift insurance liability\
for manufactures.')

for chunk in doc9.noun_chunks:
    print(chunk)

from spacy import displacy

doc = nlp(u'Apple is building India factor for $10 million')

displacy.render(doc,style='dep',jupyter=True,\
                options={"distance":110})

displacy.render(doc,style='dep',jupyter=True,\
                options={"distance":90})

doc2 = nlp(u"Over the last 6 months Apple sold over $7 billion\
iphone from India to other countries")

displacy.render(doc2,style='ent',jupyter=True)