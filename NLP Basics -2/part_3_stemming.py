# -*- coding: utf-8 -*-
"""Part -3 Stemming.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rUdK6NGNLzOEss_XZdhNzJCTQSzoqOP4

# Stemming

-> When we search text for a certain keyword, it helps if the search return variations of the word.

-> For example, searching for  "boat" might also return "boats" and "boating". Here "boat" would be the stem for [boat, boater, boating, boats]

-> Stemming is a somewhat crude method for cataloging related words; it essentially chops off letters from the end until the stem is reached.

-> This works fairly well in most cases, but unfortunately English has many exceptions where a more sphisticated process required.

# Porters' Algorithm

-> One of the most common and effective stemming tools is
Porter's Algorithm developed by Martin Porter in 1980.

-> The algorithm emoploys 5 phases  of word reduction,
each with its own set of mapping rules.

-> In the 1st phase, simple suffix mapping rules are defined such as:

S1 to S2:

1.) SSES -> SS
2.) IES -> I
3.) SS -> SS
4.) S ->



word to stem:

caresses -> caress
ponies -> poni
ties -> ti
caress -> caress
cats -> cat

-> SO from the given set of stemming rules only one rule is applied,
 based off the longest suffix, S1.

-> Thus caresses reduces to caress but not to cares.
That way there is not confusion and mixing up different
types of words to the wrong stem.

-> There are some more sophisticated phases consider
the length/ complexity of the word before applying a rule.
For example,

S1 --> S2
(m>0) ATIONAL -> ATE
(m>0) EED -> EE


WORD --> stem

relational -> relate
national -> national

agreed-> agree
feed -> feed

# Snowball

Snowball is the name of a stemming language also developed by Martin Porter.

The algorithm used here is more accurately called the "English Stemmer"
or "Porter2 Stemmer"

If offers a slight improvement over the original Porter Stemmer, both
in logic and speed.


""
"""

import nltk

from nltk.stem.porter import PorterStemmer

p_stem= PorterStemmer()

words = ["run","runner","ran","runs","Love","Malav","Maya","Mrugesh",\
         "fairly","easily","notably"]

for word in words:
    print(word + "--->"+p_stem.stem(word))
    print("\n")

from nltk.stem.snowball import SnowballStemmer

s_stem = SnowballStemmer(language='english')

for word in words:
    print(word + "--->"+s_stem.stem(word))
    print("\n")

# There is some set of algorithmic rules  that these stemmers are
# following to try to reduce down these words to some sort of
# root idea or root word.

words=['Adventure','Harmony','Breeze','Enigma','Velvet','Radiance',\
       'Whimsy','Infinite','Reverie','Cascade']

for word in words:
    print(word + " ------->"+p_stem.stem(word))

print(" ")

for word in words:
    print(word + " ------->"+s_stem.stem(word))