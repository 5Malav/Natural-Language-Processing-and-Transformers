# -*- coding: utf-8 -*-
"""Part - 5 Sentence Segmentation. ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13rVrLgTs4QIYuxz3RNw4yCr0mKnPfqPU

# Sentence Segmentation

Sentence segmentation (also called sentence boundary detection) is a key task in Natural Language Processing (NLP) where the goal is to divide a large block of text into individual sentences. This process is typically the first step in many NLP applications such as text parsing, information extraction, sentiment analysis, and machine translation.

Why is Sentence Segmentation Important?
Sentence segmentation helps to:

Break down text into manageable chunks (sentences).
Allow more accurate downstream NLP tasks such as tokenization, named entity recognition (NER), and dependency parsing.
Improve the performance of models by ensuring they operate on clearly defined sentence boundaries.
Challenges in Sentence Segmentation:
While the task seems simple, it can be challenging due to:

-> Abbreviations: Many abbreviations, such as "Dr." or "e.g.," can look like sentence boundaries but aren't. For example, "Dr. Smith is here." should not be split after "Dr.".

-> Punctuation: Full stops, exclamation points, and question marks might occur inside quotes, parentheses, or other constructs that do not mark sentence boundaries.

-> Non-English Languages: Different languages have varying punctuation rules and structures, which makes segmentation more complex across languages.


Common Approaches for Sentence Segmentation:-


1.) Rule-based Methods

Early sentence segmentation methods relied on rule-based approaches, where rules and regular expressions were defined to split text based on punctuation marks like periods, exclamation points, or question marks. However, this approach often failed to handle exceptions like abbreviations or titles (e.g., "Dr." or "Mr.").

Example rule:

Split after a period if the next character is a space and the word after the period is not an abbreviation.

2.) Machine Learning-based Methods

Machine learning models can be trained to predict sentence boundaries using labeled data. Features might include:

Punctuation marks (e.g., period, question mark, exclamation mark).
Part-of-speech (POS) tags.
Word sequences that often appear at sentence boundaries (e.g., "Mr.", "Ms.", "Dr.", "etc.").
These models can be trained using supervised learning techniques such as Conditional Random Fields (CRFs) or Decision Trees.

3.) Deep Learning-based Methods

Deep learning approaches, particularly Recurrent Neural Networks (RNNs), LSTMs (Long Short-Term Memory), and Transformer-based models, are highly effective for sentence segmentation. These models learn to recognize sentence boundaries through context and can handle more complex scenarios like nested clauses or ambiguous punctuation.

For example:

A Bidirectional LSTM model could be trained to understand sentence boundaries from both directions (before and after a word).
Transformers (e.g., BERT, GPT) have also been used in segmentation tasks, where they learn from context, making them more robust to complex sentence structures.

4.) Pre-built Libraries and Tools

Many popular NLP libraries have built-in sentence segmentation capabilities that use either rule-based or machine learning-based models. These tools often handle a wide range of edge cases.

SpaCy: A popular NLP library that comes with pre-trained models capable of sentence segmentation out of the box.
NLTK (Natural Language Toolkit): Another NLP library with a sentence tokenizer that can be used to split text into sentences.
Stanza: A Python NLP package developed by Stanford that provides sentence segmentation models.
## Hugging Face Transformers: we canfine-tune transformer models for specific NLP tasks, including sentence segmentation.
"""

import spacy
nlp = spacy.load("en_core_web_sm")

doc = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')

for sent in doc.sents:
    print(sent)

# doc.sents generatesn a sentence instead of holding them all in memory.
# That's to save space because we can imagine, a document object can be
# huge sometimes.
# so we would not want all of this saved duplicated in memory.
# Instead , we can iterate through the sentences.

# so we can grab individual token but not sentences.

list[doc.sents]

doc2 = nlp(u'"Management is doing things right; leadership is\
 doing the right things." -Peter Drucker')

for sent in doc2.sents:
    print(sent)

for sent in doc2.sents:
    print(sent)
    print("\n")

# Create a new rule.

# Add a segmentation rule

# Change segmentation rules

# Add a segmentation rule


def set_custom_boudaries(doc):
    for token in doc:
        print(token)
        print(token.i)

set_custom_boudaries(doc2)

# SO new sentence will start after semicolon ;
import spacy
from spacy.language import Language

@Language.component("custom_boundaries")  # Register the component with a name
def set_custom_boudaries(doc):
    for token in doc[:-1]:
        if token.text ==';':
            doc[token.i+1].is_sent_start=True
    return doc

"""

ValueError: [E966] `nlp.add_pipe` now takes the string name of the registered component
factory, not a callable component. Expected string, but got <function set_custom_boudaries
 at 0x79445bf83130> (name: 'None').

- If you created your component with `nlp.create_pipe('name')`: remove nlp.create_pipe and
call `nlp.add_pipe('name')` instead.

- If you passed in a component like `TextCategorizer()`: call `nlp.add_pipe` with the string
name instead, e.g. `nlp.add_pipe('textcat')`.

- If you're using a custom component: Add the decorator `@Language.component`
 (for function components) or `@Language.factory` (for class components / factories)
 to your custom component and assign it a name, e.g. `@Language.component('your_name')`.
  You can then run `nlp.add_pipe('your_name')` to add it to the pipeline.

Solution:-

The error message "ValueError: [E966] nlp.add_pipe now takes the string name of the
 registered component factory, not a callable component"
 indicates that you're trying to directly add the set_custom_boudaries
  function to the spaCy pipeline using nlp.add_pipe. However,
   spaCy now requires you to register custom components using decorators
   and then add them by their registered name.

The error happens because spaCy expects a string representing the registered
component's name, not the function itself.
"""

doc2

doc2[:-1]

nlp.add_pipe("custom_boundaries",before='parser')

nlp.pipe_names

doc4 = nlp(u'"Management is doing things right; leadership is\
 doing the right things." -Peter Drucker')

for sent in doc4.sents:
    print(sent)