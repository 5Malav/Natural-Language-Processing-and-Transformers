# -*- coding: utf-8 -*-
"""Part -3 Named Entity Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17KmkA5bjbv6lLkLgwFodzZois63bbTLA

# Named Entity Recognition

Named entity recognition(NER) seeks to locate and classify named entity
mentions in unstructured text into pre-defined categories such as the
person names organizations, locations, medical codes, time expressions,
quantities, monetary values, percentages etc.

Our goal is to read in a raw text such as:
Malav bought 300 shares of Tesla in 2020

And add addittional NER information:

[Malav] {person} bouught 300 shares of [Tesla] {organizaition}
in [2006] {time}

""

## Entity annotations
`Doc.ents` are token spans with their own set of annotations.
<table>
<tr><td>`ent.text`</td><td>The original entity text</td></tr>
<tr><td>`ent.label`</td><td>The entity type's hash value</td></tr>
<tr><td>`ent.label_`</td><td>The entity type's string description</td></tr>
<tr><td>`ent.start`</td><td>The token span's *start* index position in the Doc</td></tr>
<tr><td>`ent.end`</td><td>The token span's *stop* index position in the Doc</td></tr>
<tr><td>`ent.start_char`</td><td>The entity text's *start* index position in the Doc</td></tr>
<tr><td>`ent.end_char`</td><td>The entity text's *stop* index position in the Doc</td></tr>
</table>

## NER Tags
Tags are accessible through the `.label_` property of an entity.
<table>
<tr><th>TYPE</th><th>DESCRIPTION</th><th>EXAMPLE</th></tr>
<tr><td>`PERSON`</td><td>People, including fictional.</td><td>*Fred Flintstone*</td></tr>
<tr><td>`NORP`</td><td>Nationalities or religious or political groups.</td><td>*The Republican Party*</td></tr>
<tr><td>`FAC`</td><td>Buildings, airports, highways, bridges, etc.</td><td>*Logan International Airport, The Golden Gate*</td></tr>
<tr><td>`ORG`</td><td>Companies, agencies, institutions, etc.</td><td>*Microsoft, FBI, MIT*</td></tr>
<tr><td>`GPE`</td><td>Countries, cities, states.</td><td>*France, UAR, Chicago, Idaho*</td></tr>
<tr><td>`LOC`</td><td>Non-GPE locations, mountain ranges, bodies of water.</td><td>*Europe, Nile River, Midwest*</td></tr>
<tr><td>`PRODUCT`</td><td>Objects, vehicles, foods, etc. (Not services.)</td><td>*Formula 1*</td></tr>
<tr><td>`EVENT`</td><td>Named hurricanes, battles, wars, sports events, etc.</td><td>*Olympic Games*</td></tr>
<tr><td>`WORK_OF_ART`</td><td>Titles of books, songs, etc.</td><td>*The Mona Lisa*</td></tr>
<tr><td>`LAW`</td><td>Named documents made into laws.</td><td>*Roe v. Wade*</td></tr>
<tr><td>`LANGUAGE`</td><td>Any named language.</td><td>*English*</td></tr>
<tr><td>`DATE`</td><td>Absolute or relative dates or periods.</td><td>*20 July 1969*</td></tr>
<tr><td>`TIME`</td><td>Times smaller than a day.</td><td>*Four hours*</td></tr>
<tr><td>`PERCENT`</td><td>Percentage, including "%".</td><td>*Eighty percent*</td></tr>
<tr><td>`MONEY`</td><td>Monetary values, including unit.</td><td>*Twenty Cents*</td></tr>
<tr><td>`QUANTITY`</td><td>Measurements, as of weight or distance.</td><td>*Several kilometers, 55kg*</td></tr>
<tr><td>`ORDINAL`</td><td>"first", "second", etc.</td><td>*9th, Ninth*</td></tr>
<tr><td>`CARDINAL`</td><td>Numerals that do not fall under another type.</td><td>*2, Two, Fifty-two*</td></tr>
</table>
"""

import spacy
nlp = spacy.load("en_core_web_sm")

def show_ents(doc):
    if doc.ents:
        for ent in doc.ents:
            print(ent.text + ' - '+ ent.label_ + ' - '+ \
                  str(spacy.explain(ent.label_)))
    else:
        print("No entities found")

doc = nlp(u'Hi! I am Malav Joshi')

show_ents(doc)

doc1 = nlp(u'I have the  $100k worth of 5000 shares of different Indian companies from \
2020 to 2024')

show_ents(doc1)

# Now let's add named entity to a span.
# Now normally we would have spacy build a library of named
# entities by training it on several samples of text
# but in this case we only want to add one value.

doc2 = nlp(u'Tesla to build factory in India and will invest $2 billion')

show_ents(doc2)

from spacy.tokens import Span

ORG = doc.vocab.strings[u'ORG']

ORG

new_ent = Span(doc2,0,1,label=ORG)

new_ent

doc2.ents = list(doc2.ents) + [new_ent]

show_ents(doc2)

# SO we added Tesla as named enitity in our doc2

# Now let's try to add several terms as NERs

# For example, if we have a vaccum company, we want to add both
# vaccum cleaner and vaccum-cleanner as PROD(product) NERs

doc = nlp(u'Our company created a brand new vaccum cleaner.'\
u'This new vaccum-cleaner is the best in country right now.')

show_ents(doc)

from spacy.matcher import PhraseMatcher

matcher = PhraseMatcher(nlp.vocab)

phrase_list = ['vaccum cleaner','vaccum-cleaner']

phrase_patterns = [nlp(text) for text in phrase_list]

matcher.add("newproduct",None,*phrase_patterns)

found_matcher= matcher(doc)
found_matcher

from spacy.tokens import Span

PROD=doc.vocab.strings[u"PRODUCT"]
PROD

new_ents=[Span(doc,match[1],match[2],label=PROD) for match in found_matcher]
new_ents

doc.ents = list(doc.ents) + new_ents

show_ents(doc)

doc = nlp(u'Originally priced at $29.50,\nthe sweater was marked down to five dollars.')

show_ents(doc)

"""___
## Noun Chunks
`Doc.noun_chunks` are *base noun phrases*: token spans that include the noun and words describing the noun. Noun chunks cannot be nested, cannot overlap, and do not involve prepositional phrases or relative clauses.<br>
Where `Doc.ents` rely on the **ner** pipeline component, `Doc.noun_chunks` are provided by the **parser**.

### `noun_chunks` components:
<table>
<tr><td>`.text`</td><td>The original noun chunk text.</td></tr>
<tr><td>`.root.text`</td><td>The original text of the word connecting the noun chunk to the rest of the parse.</td></tr>
<tr><td>`.root.dep_`</td><td>Dependency relation connecting the root to its head.</td></tr>
<tr><td>`.root.head.text`</td><td>The text of the root token's head.</td></tr>
</table>
"""

print([ent for ent in doc.ents if ent.label_ == "MONEY"])

len([ent for ent in doc.ents if ent.label_ == "MONEY"])

"""### `Doc.noun_chunks` is a  generator function
Previously we mentioned that `Doc` objects do not retain a list of sentences, but they're available through the `Doc.sents` generator.<br>It's the same with `Doc.noun_chunks` - lists can be created if needed:
"""

doc = nlp(u"Autonomous cars shift insurance liability toward manufacturers.")

for chunk in doc.noun_chunks:
    print(chunk.text+' - '+chunk.root.text+' - '+chunk.root.dep_+\
          ' - '+chunk.root.head.text)

len(list(doc.noun_chunks))