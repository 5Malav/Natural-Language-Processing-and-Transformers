# -*- coding: utf-8 -*-
"""Project -1 Message Classification(Part -1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IqgfvWa2bKVzvQCfRbqE3tNCVIp4EwsN
"""



# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
# %matplotlib inline

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC

from sklearn import metrics

data=pd.read_csv("/content/smsspamcollection.tsv",sep="\t")
data.head(10)



# Check for missing data

data.isnull().sum()

data.info()

# target column
data['label'].value_counts()

plt.xscale('log')

bins = 1.15**(np.arange(0,50))

plt.hist(data[data['label']=='ham']['length'],bins=bins,alpha=0.8)

plt.hist(data[data['label']=='spam']['length'],bins=bins,alpha=0.8)

plt.legend(('ham','spam'))

plt.show()

# spam message length looks longer than ham text messages

plt.xscale('log')

bins = 1.15**(np.arange(0,50))

plt.hist(data[data['label']=='ham']['punct'],bins=bins,alpha=0.8)

plt.hist(data[data['label']=='spam']['punct'],bins=bins,alpha=0.8)

plt.legend(('ham','spam'))

plt.show()

# spam tends to have higher range of values
# spam messages tend to be longer of overall values.

# ham has quite a large range of length.
#

# X feature data

X = data[['length','punct']]

# y is label

y = data['label']

print(X.head())

print("")

print(y.head())

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,\
random_state=42)

X_train.shape

X_test.shape

y_train.shape

y_test.shape

logistic_model=LogisticRegression(solver='lbfgs')

logistic_model.fit(X_train,y_train)

pred= logistic_model.predict(X_test)
pred

cm = metrics.confusion_matrix(y_test,pred)
cm

cm = pd.DataFrame(cm,index=['ham','spam'],columns=['ham','spam'])
cm

report=metrics.classification_report(y_test,pred)
report

report=metrics.classification_report(y_test,pred,output_dict=True)
report

report_1=pd.DataFrame(report).transpose()
report_1

# For ham recall and precision is good.

# For spam recall and precision is really low.

# Overall model is good at detecting ham but poor when it comes to spam.

# Model is 84.27% accurate

"""# Naive Bayes"""

navie_bayes_model=MultinomialNB()
navie_bayes_model.fit(X_train,y_train)

pred= navie_bayes_model.predict(X_test)
print(pred)

print(" ")


print(" ")

cm = pd.DataFrame(cm,index=['ham','spam'],columns=['ham','spam'])
print(cm)

print(" ")

report=metrics.classification_report(y_test,pred)
print(report)

# For ham recall and precision is good.

# For spam recall and precision is poor.
# we are not able to identify any spam email.

# Model is 86% accurate

"""# Support vector Classifier"""

svc_model=SVC()
svc_model.fit(X_train,y_train)

pred= svc_model.predict(X_test)
print(pred)

print(" ")


print(" ")

cm = pd.DataFrame(cm,index=['ham','spam'],columns=['ham','spam'])
print(cm)

print(" ")

report=metrics.classification_report(y_test,pred)
print(report)

# For ham recall and precision is good.

# For spam recall and precision is better compare to
# logistic regression and Naive bayes.

# Using Support vector classifier we get better result
# to identify spam and ham emails.

# Model is 86% accurate



















